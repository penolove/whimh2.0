{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [Sec1] forward pretrianed-vggface19 model\n",
    "## if mem less than 20GB directly go sec2\n",
    "## read pretrained vgg face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "with open(\"vggface16.tfmodel\", mode='rb') as f:\n",
    "  fileContent = f.read()\n",
    "\n",
    "graph_def = tf.GraphDef()\n",
    "graph_def.ParseFromString(fileContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tf = tf.placeholder(tf.float32, shape=[None, 32,32,3])\n",
    "x_tf_1=tf.image.resize_images(x_tf,224,224)\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,])\n",
    "tf.import_graph_def(graph_def, input_map={ \"images\": x_tf_1 })\n",
    "print \"graph loaded from disk\"\n",
    "\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.train.SummaryWriter('/tmp/loser3/train',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.graph.get_operations()[88].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this will get all operation in graph\n",
    "sess.graph.get_operations()\n",
    "#we can get specific operation by\n",
    "graph.get_tensor_by_name(\"import/Relu_1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#what we want is the out come from fc_7\n",
    "feature_net=graph.get_tensor_by_name(\"import/pool5:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames=np.load(\"outfile_x.npy\")\n",
    "y_train=np.load(\"outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x1 = sess.run([feature_net], feed_dict={x_tf:frames[:100], y_tf: y_train[:100]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trial = sess.run([x_tf_1], feed_dict={x_tf:frames[:100], y_tf: y_train[:100]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(trial[3].astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward pass all pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x1 = sess.run([feature_net], feed_dict={x_tf:frames[:100], y_tf: y_train[:100]})[0]\n",
    "feature_x2 = sess.run([feature_net], feed_dict={x_tf:frames[100:200], y_tf: y_train[100:200]})[0]\n",
    "feature_x3 = sess.run([feature_net], feed_dict={x_tf:frames[200:300], y_tf: y_train[200:300]})[0]\n",
    "feature_x4 = sess.run([feature_net], feed_dict={x_tf:frames[300:400], y_tf: y_train[300:400]})[0]\n",
    "feature_x5 = sess.run([feature_net], feed_dict={x_tf:frames[400:500], y_tf: y_train[400:500]})[0]\n",
    "feature_x6 = sess.run([feature_net], feed_dict={x_tf:frames[500:600], y_tf: y_train[500:600]})[0]\n",
    "feature_x7 = sess.run([feature_net], feed_dict={x_tf:frames[600:700], y_tf: y_train[600:700]})[0]\n",
    "feature_x8 = sess.run([feature_net], feed_dict={x_tf:frames[700:800], y_tf: y_train[700:800]})[0]\n",
    "feature_x9 = sess.run([feature_net], feed_dict={x_tf:frames[800:900], y_tf: y_train[800:900]})[0]\n",
    "feature_x10 = sess.run([feature_net], feed_dict={x_tf:frames[900:], y_tf: y_train[900:]})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_x=(np.concatenate((feature_x1, feature_x2, feature_x3,\n",
    "                    feature_x4, feature_x5, feature_x6,\n",
    "                    feature_x7, feature_x8, feature_x9,\n",
    "                    feature_x10), axis=0))\n",
    "print feature_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"/home/stream/whimh2.0/finetune\",feature_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "#norm2 = normalize(feature_x, axis=1)\n",
    "arr = np.arange(len(feature_x))\n",
    "np.random.shuffle(arr)\n",
    "x_train=feature_x[arr]\n",
    "#x_train=norm2[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [Sec2] retrain WHIMH\n",
    "# now train softmax model with (?,7,7,512) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_x=np.load(\"finetune.npy\")\n",
    "y_train=np.load(\"outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 770 images in training set\n",
      "there are 193 images in testing set\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.preprocessing import normalize\n",
    "#norm2 = normalize(feature_x, axis=1)\n",
    "arr = np.arange(len(feature_x))\n",
    "np.random.shuffle(arr)\n",
    "x_train=feature_x[arr]\n",
    "#x_train=norm2[arr]\n",
    "y_train=y_train[arr]\n",
    "data={\n",
    "  'X_train': x_train[:int(len(x_train)*0.8)],\n",
    "  'y_train': y_train[:int(len(x_train)*0.8)],\n",
    "  'X_val': x_train[int(len(x_train)*0.8):],\n",
    "  'y_val': y_train[int(len(x_train)*0.8):],\n",
    "}\n",
    "print \"there are \"+ str(data['X_train'].shape[0]) + \" images in training set\"\n",
    "print \"there are \"+ str(data['X_val'].shape[0]) + \" images in testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.0001)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.01, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tfed = tf.placeholder(tf.float32, shape=[None,7,7,512],name='feature_x')\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name='truth_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('fintune_whimh'):\n",
    "    with tf.name_scope('finetune_layer'):\n",
    "        #calculate_entropy\n",
    "        h_pool3_flat = tf.reshape(x_tfed, [-1, 7*7*512])\n",
    "        W_whimh_fc1 = weight_variable([7*7*512, 1024])\n",
    "        b_whimh_fc1 = bias_variable([1024])\n",
    "        h_fc1=tf.nn.relu(tf.matmul(h_pool3_flat, W_whimh_fc1) + b_whimh_fc1) \n",
    "        tf.histogram_summary('fc_1/weights', W_whimh_fc1)\n",
    "    with tf.name_scope('finetune_layer'):\n",
    "        #calculate_entropy\n",
    "        W_whimh_fc2 = weight_variable([1024, 3])\n",
    "        b_whimh_fc2 = bias_variable([3])\n",
    "        y_conv=tf.nn.softmax(tf.matmul(h_fc1, W_whimh_fc2) + b_whimh_fc2) \n",
    "        tf.histogram_summary('fc_2/weights', W_whimh_fc2)\n",
    "    y_tf_2=tf.one_hot(y_tf,3)\n",
    "    with tf.name_scope('loss_calulate'):\n",
    "        #calculate_entropy\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_tf_2 * tf.log(y_conv), reduction_indices=[1]))\n",
    "    with tf.name_scope('regular'):    \n",
    "        regularizers = (tf.nn.l2_loss(W_whimh_fc1) + tf.nn.l2_loss(b_whimh_fc1)+\n",
    "                        (tf.nn.l2_loss(W_whimh_fc2) + tf.nn.l2_loss(b_whimh_fc2))\n",
    "                       )\n",
    "\n",
    "    with tf.name_scope('loss'):  \n",
    "        loss=cross_entropy+1e-3*regularizers\n",
    "    with tf.name_scope('slover'):\n",
    "        train_step = tf.train.AdamOptimizer(1e-6).minimize(loss)\n",
    "    with tf.name_scope('measure'):\n",
    "        with tf.name_scope('predict'):\n",
    "            predctions=tf.argmax(y_conv,1)\n",
    "        with tf.name_scope('groundtruth'):\n",
    "            ground_truth=tf.argmax(y_tf_2,1)\n",
    "        correct_prediction = tf.equal(predctions, ground_truth)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        #MHaccuracy\n",
    "        idxsMH = tf.where(tf.equal(ground_truth, 0))\n",
    "        correct_predictionMH = tf.equal(tf.argmax(tf.gather(y_conv,idxsMH),2), tf.argmax(tf.gather(y_tf_2,idxsMH),2))\n",
    "        accuracyMH = tf.reduce_mean(tf.cast(correct_predictionMH, tf.float32))\n",
    "        #NMaccuracy\n",
    "        idxsNM = tf.where(tf.equal(ground_truth, 1))\n",
    "        correct_predictionNM = tf.equal(tf.argmax(tf.gather(y_conv,idxsNM),2), tf.argmax(tf.gather(y_tf_2,idxsNM),2))\n",
    "        accuracyNM = tf.reduce_mean(tf.cast(correct_predictionNM, tf.float32))\n",
    "        #NHaccuracy\n",
    "        idxsNH = tf.where(tf.equal(ground_truth, 2))\n",
    "        correct_predictionNH = tf.equal(tf.argmax(tf.gather(y_conv,idxsNH),2), tf.argmax(tf.gather(y_tf_2,idxsNH),2))\n",
    "        accuracyNH = tf.reduce_mean(tf.cast(correct_predictionNH, tf.float32))\n",
    "\n",
    "    tf.scalar_summary('accuracy', accuracy)\n",
    "    tf.scalar_summary('accuracyMH', accuracyMH)\n",
    "    tf.scalar_summary('accuracyNM', accuracyNM)\n",
    "    tf.scalar_summary('accuracyNH', accuracyNH)\n",
    "    #this variable means records all records\n",
    "    merged = tf.merge_all_summaries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.train.SummaryWriter('/tmp/loser3/train',sess.graph)\n",
    "test_writer = tf.train.SummaryWriter('/tmp/loser3/test',sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.632468, testing acc 0.57513\n",
      "step 10, training accuracy 0.928571, testing acc 0.932642\n",
      "step 20, training accuracy 0.963636, testing acc 0.948187\n",
      "step 30, training accuracy 0.983117, testing acc 0.963731\n",
      "step 40, training accuracy 0.990909, testing acc 0.968912\n",
      "step 50, training accuracy 0.993506, testing acc 0.963731\n",
      "step 60, training accuracy 0.997403, testing acc 0.979275\n",
      "step 70, training accuracy 0.998701, testing acc 0.979275\n",
      "step 80, training accuracy 0.998701, testing acc 0.979275\n",
      "step 90, training accuracy 0.998701, testing acc 0.979275\n",
      "step 100, training accuracy 0.998701, testing acc 0.979275\n",
      "step 110, training accuracy 1, testing acc 0.979275\n",
      "step 120, training accuracy 1, testing acc 0.979275\n",
      "step 130, training accuracy 1, testing acc 0.979275\n",
      "step 140, training accuracy 1, testing acc 0.974093\n",
      "step 150, training accuracy 1, testing acc 0.974093\n",
      "step 160, training accuracy 1, testing acc 0.974093\n",
      "step 170, training accuracy 1, testing acc 0.974093\n",
      "step 180, training accuracy 1, testing acc 0.974093\n",
      "step 190, training accuracy 1, testing acc 0.974093\n",
      "step 200, training accuracy 1, testing acc 0.974093\n",
      "step 210, training accuracy 1, testing acc 0.974093\n",
      "step 220, training accuracy 1, testing acc 0.974093\n",
      "step 230, training accuracy 1, testing acc 0.974093\n",
      "step 240, training accuracy 1, testing acc 0.974093\n",
      "step 250, training accuracy 1, testing acc 0.974093\n",
      "step 260, training accuracy 1, testing acc 0.974093\n",
      "step 270, training accuracy 1, testing acc 0.974093\n",
      "step 280, training accuracy 1, testing acc 0.974093\n",
      "step 290, training accuracy 1, testing acc 0.974093\n",
      "step 300, training accuracy 1, testing acc 0.974093\n",
      "step 310, training accuracy 1, testing acc 0.974093\n",
      "step 320, training accuracy 1, testing acc 0.974093\n",
      "step 330, training accuracy 1, testing acc 0.974093\n",
      "step 340, training accuracy 1, testing acc 0.974093\n",
      "step 350, training accuracy 1, testing acc 0.974093\n",
      "step 360, training accuracy 1, testing acc 0.974093\n",
      "step 370, training accuracy 1, testing acc 0.974093\n",
      "step 380, training accuracy 1, testing acc 0.974093\n",
      "step 390, training accuracy 1, testing acc 0.974093\n",
      "step 400, training accuracy 1, testing acc 0.974093\n",
      "step 410, training accuracy 1, testing acc 0.974093\n",
      "step 420, training accuracy 1, testing acc 0.974093\n",
      "step 430, training accuracy 1, testing acc 0.974093\n",
      "step 440, training accuracy 1, testing acc 0.974093\n",
      "step 450, training accuracy 1, testing acc 0.974093\n",
      "step 460, training accuracy 1, testing acc 0.974093\n",
      "step 470, training accuracy 1, testing acc 0.974093\n",
      "step 480, training accuracy 1, testing acc 0.974093\n",
      "step 490, training accuracy 1, testing acc 0.974093\n",
      "test accuracy 0.974093\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in range(500):\n",
    "  arr=np.arange(len(data['X_train']))\n",
    "  np.random.shuffle(arr)\n",
    "  idx = arr[:60]\n",
    "  train_step.run(feed_dict={x_tfed:data['X_train'][idx], y_tf: data['y_train'][idx]})\n",
    "  if i%10==0:\n",
    "    ############training acc###########\n",
    "    #train_accuracy = accuracy.eval(feed_dict={   \n",
    "    #x_tf:data['X_train'], y_tf: data['y_train'], keep_prob: 1.0})\n",
    "    summary, train_accuracy = sess.run([merged, accuracy], feed_dict={   \n",
    "            x_tfed:data['X_train'], y_tf: data['y_train']})\n",
    "    train_writer.add_summary(summary, i)\n",
    "    ############testing acc#########\n",
    "    #testing_accuracy = accuracy.eval(feed_dict={   \n",
    "    #x_tf:data['X_val'], y_tf: data['y_val'], keep_prob: 1.0})\n",
    "    \n",
    "    summary, testing_accuracy = sess.run([merged, accuracy], feed_dict={   \n",
    "            x_tfed:data['X_val'], y_tf: data['y_val']})\n",
    "    test_writer.add_summary(summary, i)\n",
    "    print(\"step %d, training accuracy %g, testing acc %g\"%(i, train_accuracy,testing_accuracy))\n",
    "    \n",
    "#overall\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x_tfed:data['X_val'], y_tf: data['y_val']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PretensorWhimh.ckpt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#proto\n",
    "#for sec [3,4]\n",
    "tf.train.write_graph(sess.graph_def, \"/home/stream/whimh2\", \"pretensorWhimh.pb\", False) \n",
    "#save weights\n",
    "saver=tf.train.Saver(tf.all_variables())\n",
    "saver.save(sess,\"PretensorWhimh.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [sec 3] conncate these two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sess = tf.InteractiveSession()\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph loaded from disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"vggface16.tfmodel\", mode='rb') as f:\n",
    "  fileContent = f.read()\n",
    "\n",
    "graph_def = tf.GraphDef()\n",
    "graph_def.ParseFromString(fileContent)\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None, 32,32,3],name=\"raw_images_mh\")\n",
    "x_tf_1=tf.image.resize_images(x_tf,224,224)\n",
    "y_tf = tf.placeholder(tf.int32, shape=[None,],name=\"turth_y_mh\")\n",
    "feature_x=tf.import_graph_def(graph_def,name='vggface', input_map={ \"images\": x_tf_1 },return_elements=[\"pool5:0\"])\n",
    "print \"graph loaded from disk\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsaver=tf.train.import_meta_graph('PretensorWhimh.ckpt.meta')\n",
    "newsaver.restore(sess,'PretensorWhimh.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "pool5=graph.get_tensor_by_name(\"feature_x:0\")\n",
    "results=graph.get_tensor_by_name(\"fintune_whimh/measure/predict/ArgMax:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'raw_images_mh:0' shape=(?, 32, 32, 3) dtype=float32>,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.graph.get_operations()[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "frames=np.load(\"outfile_x.npy\")\n",
    "y_train=np.load(\"outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 1 1 1 2 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 1, 1, 1, 2, 2, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(len(frames))\n",
    "np.random.shuffle(arr)\n",
    "print y_train [arr[:10]]\n",
    "pool_x=sess.run(feature_x, feed_dict={x_tf:frames[arr[:10]]})[0]\n",
    "sess.run(results, feed_dict={pool5:pool_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_writer = tf.train.SummaryWriter('/tmp/loser/train',sess.graph)\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
